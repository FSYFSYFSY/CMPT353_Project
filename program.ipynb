{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from pykalman import KalmanFilter\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "#import revelant files\n",
    "from data_smoothing import output_kalman, kalmanSmooth, lowess_smooth, fft_denoise, apply_fft_denoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'original_data'\n",
    "#folder_path = 'fft_denoised'\n",
    "#folder_path = 'kalman_smoothed'\n",
    "#folder_path = 'lowess_smoothed'\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "run_df = pd.DataFrame()\n",
    "walk_df = pd.DataFrame()\n",
    "jump_df = pd.DataFrame()\n",
    "still_df = pd.DataFrame()\n",
    "multi_df = pd.DataFrame()\n",
    "\n",
    "run_files = list()\n",
    "walk_files = list()\n",
    "jump_files = list()\n",
    "still_files = list()\n",
    "multi_files = list()\n",
    "\n",
    "\n",
    "# We have at least 5 motion labels\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    if 'run' in file.lower():\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        run_df = pd.concat([run_df, temp_df], ignore_index=True)\n",
    "        run_files.append(temp_df)\n",
    "    elif 'walk' in file.lower():\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        walk_df = pd.concat([walk_df, temp_df], ignore_index=True)\n",
    "        walk_files.append(temp_df)\n",
    "    elif 'jump' in file.lower():\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        jump_df = pd.concat([jump_df, temp_df], ignore_index=True)\n",
    "        jump_files.append(temp_df)\n",
    "    elif 'still' in file.lower():\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        still_df = pd.concat([still_df, temp_df], ignore_index=True)\n",
    "        still_files.append(temp_df)\n",
    "    elif 'multi' in file.lower():\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        multi_df = pd.concat([multi_df, temp_df], ignore_index=True)\n",
    "        multi_files.append(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have process the data into dedicated folder, no need do it again\n",
    "#After we smoothed the data, we need to load new data again\n",
    "\n",
    "# Function to process each file group\n",
    "def process_file_group(file_group, label):\n",
    "    for i, df in enumerate(file_group, start=1):\n",
    "        file_name = f'{label}{i}'\n",
    "        lowess_smooth(df, label, file_name)\n",
    "        output_kalman(df, label, file_name)\n",
    "        apply_fft_denoise(df, label, file_name)\n",
    "        \n",
    "# Dictionary mapping labels to file lists\n",
    "file_groups = {\n",
    "    'run': run_files,\n",
    "    'walk': walk_files,\n",
    "    'still': still_files,\n",
    "    'jump': jump_files,\n",
    "    'multi': multi_files\n",
    "}\n",
    "\n",
    "# Process each file group\n",
    "for label, files in file_groups.items():\n",
    "    process_file_group(files, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig1 = plt.figure(figsize=(10, 8))\n",
    "ax = fig1.add_subplot(111, projection='3d')\n",
    "\n",
    "sc1 = ax.scatter(walk_df['ax'], walk_df['ay'], walk_df['az'], c=walk_df['time'], cmap='viridis',marker='o', s=0.3)\n",
    "\n",
    "cbar = plt.colorbar(sc1, ax=ax, pad=0.1)\n",
    "cbar.set_label('Time')\n",
    "\n",
    "ax.set_title('Walk accelerate')\n",
    "ax.set_xlabel('Ax')\n",
    "ax.set_ylabel('Ay')\n",
    "ax.set_zlabel('Az')\n",
    "\n",
    "plt.show(fig1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, len(walk_df['time']))\n",
    "\n",
    "for i, col in enumerate(walk_df.columns[1:5]):\n",
    "    plt.subplot(4, 1, i+1)\n",
    "    plt.plot(x, walk_df[col],marker='', linestyle='-')\n",
    "    plt.title(f'{col}')\n",
    "    \n",
    "plt.suptitle('Walk Overview', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "ax = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "sc2 = ax.scatter(run_df['ax'], run_df['ay'], run_df['az'], c=run_df['time'], cmap='viridis',marker='o', s = 0.3)\n",
    "\n",
    "cbar = plt.colorbar(sc2, ax=ax, pad=0.1)\n",
    "cbar.set_label('Time')\n",
    "\n",
    "ax.set_title('Run accelerate')\n",
    "ax.set_xlabel('Ax')\n",
    "ax.set_ylabel('Ay')\n",
    "ax.set_zlabel('Az')\n",
    "\n",
    "plt.show(fig2)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, len(run_df['time']))\n",
    "\n",
    "for i, col in enumerate(run_df.columns[1:5]):\n",
    "    plt.subplot(4, 1, i+1)\n",
    "    plt.plot(x, run_df[col],marker='', linestyle='-')\n",
    "    plt.title(f'{col}')\n",
    "\n",
    "plt.suptitle('Run Overview', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(10, 8))\n",
    "ax = fig3.add_subplot(111, projection='3d')\n",
    "\n",
    "sc3 = ax.scatter(still_df['ax'], still_df['ay'], still_df['az'], c=still_df['time'], cmap='viridis',marker='o', s = 0.3)\n",
    "\n",
    "cbar = plt.colorbar(sc3, ax=ax, pad=0.1)\n",
    "cbar.set_label('Time')\n",
    "\n",
    "ax.set_title('Still accelerate')\n",
    "ax.set_xlabel('Ax')\n",
    "ax.set_ylabel('Ay')\n",
    "ax.set_zlabel('Az')\n",
    "\n",
    "plt.show(fig3)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, len(still_df['time']))\n",
    "\n",
    "for i, col in enumerate(still_df.columns[1:5]):\n",
    "    plt.subplot(4, 1, i+1)\n",
    "    plt.plot(x, still_df[col],marker='', linestyle='-')\n",
    "    plt.title(f'{col}')\n",
    "    \n",
    "plt.suptitle('Still Overview', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4 = plt.figure(figsize=(10, 8))\n",
    "ax = fig4.add_subplot(111, projection='3d')\n",
    "\n",
    "sc4 = ax.scatter(jump_df['ax'], jump_df['ay'], jump_df['az'], c=jump_df['time'], cmap='viridis',marker='o', s = 0.3)\n",
    "\n",
    "cbar = plt.colorbar(sc4, ax=ax, pad=0.1)\n",
    "cbar.set_label('Time')\n",
    "\n",
    "ax.set_title('Jump accelerate')\n",
    "ax.set_xlabel('Ax')\n",
    "ax.set_ylabel('Ay')\n",
    "ax.set_zlabel('Az')\n",
    "\n",
    "plt.show(fig4)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, len(jump_df['time']))\n",
    "\n",
    "for i, col in enumerate(jump_df.columns[1:5]):\n",
    "    plt.subplot(4, 1, i+1)\n",
    "    plt.plot(x, jump_df[col],marker='', linestyle='-')\n",
    "    plt.title(f'{col}')\n",
    "    \n",
    "plt.suptitle('Jump Overview', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5 = plt.figure(figsize=(10, 8))\n",
    "ax = fig5.add_subplot(111, projection='3d')\n",
    "\n",
    "sc5 = ax.scatter(multi_df['ax'], multi_df['ay'], multi_df['az'], c=multi_df['time'], cmap='viridis',marker='o', s = 0.3)\n",
    "\n",
    "cbar = plt.colorbar(sc5, ax=ax, pad=0.1)\n",
    "cbar.set_label('Time')\n",
    "\n",
    "ax.set_title('Multi accelerate')\n",
    "ax.set_xlabel('Ax')\n",
    "ax.set_ylabel('Ay')\n",
    "ax.set_zlabel('Az')\n",
    "\n",
    "plt.show(fig5)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, len(multi_df['time']))\n",
    "\n",
    "for i, col in enumerate(multi_df.columns[1:5]):\n",
    "    plt.subplot(4, 1, i+1)\n",
    "    plt.plot(x, multi_df[col],marker='', linestyle='-')\n",
    "    plt.title(f'{col}')\n",
    "    \n",
    "plt.suptitle('Multi Overview', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_combined = pd.concat([run_df, walk_df, jump_df, still_df, multi_df], ignore_index=True)\n",
    "X = dfs_combined[['time','ax','ay','az','speed']]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(dfs_combined['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training + Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#X_train_scaled = X_train\n",
    "#X_test_scaled = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = MLPClassifier(\n",
    "        solver = 'adam', hidden_layer_sizes=(16,32,64,128), activation='relu'\n",
    "    )\n",
    "\n",
    "model_nn.fit(X_train_scaled, y_train)\n",
    "y_pred_nn = model_nn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "print(f\"Accuracy: {accuracy_nn:.4f}\")\n",
    "\n",
    "# Print confusion matrix\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred_nn))\n",
    "cm = confusion_matrix(y_test, y_pred_nn)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NB =  GaussianNB()\n",
    "model_NB.fit(X_train_scaled, y_train)\n",
    "y_pred_NB = model_NB.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_NB = accuracy_score(y_test, y_pred_NB)\n",
    "print(f\"Accuracy: {accuracy_NB:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred_NB))\n",
    "cm = confusion_matrix(y_test, y_pred_nn)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=10)\n",
    "model_knn.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = model_knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"Accuracy: {accuracy_knn:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred_knn))\n",
    "cm = confusion_matrix(y_test, y_pred_nn)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "\n",
    "model_RF.fit(X_train_scaled, y_train)\n",
    "y_pred_RF = model_RF.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_RF = accuracy_score(y_test, y_pred_RF)\n",
    "print(f\"Accuracy: {accuracy_RF:.4f}\")\n",
    "\n",
    "# Print confusion matrix\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred_RF))\n",
    "cm = confusion_matrix(y_test, y_pred_nn)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
